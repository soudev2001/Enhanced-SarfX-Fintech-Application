import os
import uvicorn
import requests
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
import yfinance as yf
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pymongo import MongoClient
import warnings
from statsmodels.tsa.arima.model import ARIMA
from prophet import Prophet
from sklearn.metrics import mean_absolute_error
import logging

# --- CONFIGURATION (PROD) ---
warnings.filterwarnings("ignore")
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Certificat SSL pour MongoDB Atlas (Linux servers)
try:
    import certifi
    CA_BUNDLE = certifi.where()
except ImportError:
    CA_BUNDLE = None

# Identifiants (Dans une vraie prod, utilisez des variables d'environnement)
MONGO_URI = os.environ.get("MONGO_URI", "mongodb+srv://soufiane:gogo@cluster0.05omqhe.mongodb.net/SarfX_Enhanced")
SMTP_EMAIL = "starkxgroup@gmail.com"
SMTP_PASSWORD = "mpnkmpqeypjsvern"
SMTP_SERVER = "smtp.gmail.com"
SMTP_PORT = 587
ADMIN_EMAIL = "starkxgroup@gmail.com"
COINAPI_KEY = os.environ.get("COINAPI_KEY", "VOTRE_API_KEY_ICI") # Placeholder
AI_PORT = int(os.environ.get("AI_PORT", 8087))

# Cache en m√©moire pour les taux (√©vite les appels r√©p√©t√©s)
# TODO: Remplacer par Redis pour un vrai cache distribu√© en production
RATE_CACHE = {}
CACHE_TTL = 60  # secondes

# --- INITIALISATION ---
app = FastAPI(title="SarfX Core Engine", description="Moteur Fintech d'Arbitrage & IA")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Connexion Base de Donn√©es (Persistance)
try:
    # Configuration avec certificat SSL pour serveurs Linux
    mongo_options = {
        "serverSelectionTimeoutMS": 5000,
        "connectTimeoutMS": 10000,
        "retryWrites": True
    }
    if CA_BUNDLE:
        mongo_options["tlsCAFile"] = CA_BUNDLE

    client = MongoClient(MONGO_URI, **mongo_options)
    # Test de connexion
    client.admin.command('ping')
    db = client.get_database("SarfX_Enhanced")
    rates_collection = db.rates_history
    print("‚úÖ [MongoDB] Connect√© √† SarfX_Enhanced avec succ√®s.")
except Exception as e:
    print(f"‚ö†Ô∏è [MongoDB] Mode hors-ligne activ√©: {e}")
    rates_collection = None
    db = None

# --- COUCHE D'ACQUISITION (INPUT LAYER) ---

def get_cached_rate(cache_key):
    """R√©cup√®re un taux du cache si disponible et valide"""
    if cache_key in RATE_CACHE:
        cached_data = RATE_CACHE[cache_key]
        if (datetime.utcnow() - cached_data['timestamp']).total_seconds() < CACHE_TTL:
            logging.info(f"‚úì Cache hit pour {cache_key}")
            return cached_data['rate']
    return None

def set_cached_rate(cache_key, rate):
    """Enregistre un taux dans le cache"""
    RATE_CACHE[cache_key] = {
        'rate': rate,
        'timestamp': datetime.utcnow()
    }

def fetch_fiat_rate(base, target):
    """
    Source 1 : March√© Interbancaire (Frankfurter/Yahoo)

    TODO: Ajouter support pour d'autres APIs de taux de change:
    - ExchangeRate-API.com (gratuit, 1500 req/mois)
    - Fixer.io (freemium)
    - Open Exchange Rates
    """
    cache_key = f"fiat_{base}_{target}"
    cached = get_cached_rate(cache_key)
    if cached:
        return cached

    try:
        # Priorit√© API Rapide
        url = f"https://api.frankfurter.app/latest?from={base}&to={target}"
        resp = requests.get(url, timeout=2)
        if resp.status_code == 200:
            rate = resp.json()['rates'][target]
            set_cached_rate(cache_key, rate)
            logging.info(f"‚úì Taux {base}/{target} r√©cup√©r√©: {rate} (Frankfurter)")
            return rate
    except Exception as e:
        logging.warning(f"Frankfurter API √©chec: {e}")

    # Fallback Yahoo Finance (Plus lent mais robuste)
    try:
        ticker = f"{base}{target}=X"
        df = yf.download(ticker, period="1d", interval="1m", progress=False)
        if not df.empty:
            rate = df['Close'].iloc[-1]
            set_cached_rate(cache_key, rate)
            logging.info(f"‚úì Taux {base}/{target} r√©cup√©r√©: {rate} (Yahoo Finance)")
            return rate
    except Exception as e:
        logging.error(f"Yahoo Finance √©chec: {e}")

    logging.error(f"‚úó Impossible de r√©cup√©rer le taux {base}/{target}")
    return 0.0 # Echec

def fetch_crypto_implied_rate(base, target):
    """
    Source 2 : March√© Crypto (USDT Implied Rate)

    TODO: Int√©grer vraies APIs crypto au lieu de simulation:
    - Binance P2P API pour les taux USDT/MAD r√©els
    - CoinGecko API pour les prix crypto
    - Kraken API pour volumes et liquidit√©

    NOTE: Actuellement simule une prime crypto de +1.5% sur le taux fiat
    """

    fiat_rate = fetch_fiat_rate(base, target)
    if fiat_rate == 0:
        return 0.0

    # TODO: Remplacer par appel API Binance P2P r√©el
    # Exemple: GET https://p2p.binance.com/bapi/c2c/v2/friendly/c2c/adv/search
    # Params: fiat=MAD, tradeType=BUY, asset=USDT

    # Simulation d'une prime de march√© crypto (ex: +1.5% vs officiel)
    crypto_premium = 1.015
    rate = fiat_rate * crypto_premium
    logging.info(f"‚úì Taux crypto simul√© {base}/{target}: {rate} (Premium: +1.5%)")
    return rate

# --- COUCHE DE TRAITEMENT (PROCESS LAYER) ---

def calculate_best_execution(base, target, amount):
    """
    C≈ìur de l'Arbitrage : Trouve le meilleur chemin pour l'argent

    TODO: Am√©liorer l'arbitrage avec:
    - Calcul des frais r√©els par provider (bank, wise, western union, etc.)
    - Prise en compte des d√©lais de livraison
    - Score de fiabilit√© des providers (bas√© sur historique)
    - Optimisation multi-routes (ex: EUR->USD->MAD si meilleur)
    """

    # 1. Acquisition
    rate_fiat = fetch_fiat_rate(base, target)
    rate_crypto = fetch_crypto_implied_rate(base, target)
    rate_bank = rate_fiat * 0.975 # Les banques prennent ~2.5% de marge

    # 2. Arbitrage
    best_source_rate = max(rate_fiat, rate_crypto)
    source_name = "March√© Crypto (USDT)" if rate_crypto > rate_fiat else "March√© Interbancaire"

    # 3. Marge SarfX (Dynamique)
    # TODO: Rendre la marge dynamique selon:
    # - Volume de la transaction (plus le volume est gros, plus la marge baisse)
    # - Volatilit√© du march√© (augmenter en p√©riode instable)
    # - Fid√©lit√© client (r√©compenser les utilisateurs r√©guliers)
    sarfx_margin = 0.005 # 0.5% (Tr√®s comp√©titif)
    sarfx_rate = best_source_rate * (1 - sarfx_margin)

    logging.info(f"‚úì Arbitrage {base}/{target}: Meilleur taux = {sarfx_rate:.4f} ({source_name})")

    return {
        "rates": {
            "bank": rate_bank,
            "market": rate_fiat,
            "crypto": rate_crypto,
            "sarfx": sarfx_rate
        },
        "best_source": source_name,
        "savings": (amount * sarfx_rate) - (amount * rate_bank)
    }

# --- MOTEUR IA (PREDICTION LAYER) ---

def train_arima_model(df, order=(5, 1, 0)):
    """
    Entra√Æne un mod√®le ARIMA sur les donn√©es historiques

    ARIMA (AutoRegressive Integrated Moving Average):
    - AR(p): r√©gression sur p valeurs pass√©es
    - I(d): diff√©renciation d'ordre d pour stationnarit√©
    - MA(q): moyenne mobile sur q erreurs pass√©es

    TODO: Optimiser les hyperparam√®tres (p,d,q) avec auto_arima ou grid search
    """
    try:
        model = ARIMA(df['Close'], order=order)
        fitted = model.fit()
        return fitted
    except Exception as e:
        logging.error(f"ARIMA training failed: {e}")
        return None

def train_prophet_model(df):
    """
    Entra√Æne un mod√®le Prophet (Meta/Facebook) sur les donn√©es historiques

    Prophet est sp√©cialis√© dans les s√©ries temporelles avec:
    - Tendances non-lin√©aires
    - Saisonnalit√© multiple (jour/semaine/ann√©e)
    - Gestion des jours f√©ri√©s
    - Robuste aux donn√©es manquantes

    TODO: Ajouter des r√©gresseurs externes (√©v√©nements √©conomiques, news, etc.)
    """
    try:
        # Prophet n√©cessite colonnes 'ds' (date) et 'y' (valeur)
        prophet_df = pd.DataFrame({
            'ds': df.index,
            'y': df['Close']
        })

        model = Prophet(
            daily_seasonality=True,
            weekly_seasonality=True,
            yearly_seasonality=False,  # Pas assez de donn√©es historiques g√©n√©ralement
            changepoint_prior_scale=0.05  # Flexibilit√© des tendances
        )
        model.fit(prophet_df)
        return model
    except Exception as e:
        logging.error(f"Prophet training failed: {e}")
        return None

def predict_with_arima(model, steps=7):
    """Pr√©diction ARIMA pour N jours"""
    try:
        forecast = model.forecast(steps=steps)
        return forecast.tolist()
    except Exception as e:
        logging.error(f"ARIMA prediction failed: {e}")
        return None

def predict_with_prophet(model, steps=7):
    """Pr√©diction Prophet pour N jours"""
    try:
        future = model.make_future_dataframe(periods=steps)
        forecast = model.predict(future)
        # Retourner seulement les pr√©dictions futures
        return forecast['yhat'].tail(steps).tolist()
    except Exception as e:
        logging.error(f"Prophet prediction failed: {e}")
        return None

def generate_ai_signal(pair):
    """
    Analyse la tendance pour donner un conseil (Timing)

    TODO: Enrichir l'analyse avec:
    - Indicateurs techniques (RSI, MACD, Bollinger Bands)
    - Analyse de sentiment (Twitter, news financi√®res)
    - D√©tection d'anomalies (spikes inhabituels)
    - Scoring de confiance bas√© sur plusieurs indicateurs
    """
    ticker = f"{pair}=X"
    try:
        df = yf.download(ticker, period="1mo", interval="1d", progress=False)
        if df.empty:
            return "NEUTRE"

        # Analyse simple : Moyenne Mobile Exponentielle (EMA)
        short_window = 5
        df['EMA'] = df['Close'].ewm(span=short_window, adjust=False).mean()

        last_close = df['Close'].iloc[-1]
        last_ema = df['EMA'].iloc[-1]

        # Logique de signal
        if last_close < last_ema * 0.995: # Prix significativement sous la moyenne -> Potentiel rebond
            signal = "ACHETER (Taux bas)"
        elif last_close > last_ema * 1.005: # Prix haut -> Attendre correction
            signal = "ATTENDRE (Taux haut)"
        else:
            signal = "NEUTRE"

        logging.info(f"‚úì Signal IA pour {pair}: {signal}")
        return signal
    except Exception as e:
        logging.error(f"AI signal generation failed: {e}")
        return "INDISPONIBLE"

# --- T√ÇCHES DE FOND (BACKGROUND) ---

def archive_rate(base, target, rate, source):
    """
    Historisation dans MongoDB pour l'apprentissage futur

    TODO: Utiliser ces donn√©es pour:
    - Entra√Æner des mod√®les ML p√©riodiquement (batch nightly)
    - Calculer des statistiques de performance (accuracy vs reality)
    - D√©tecter des patterns d'arbitrage r√©currents
    - Dashboard analytics pour admin
    """
    if rates_collection is None:
        return
    try:
        rates_collection.insert_one({
            "pair": f"{base}/{target}",
            "rate": float(rate),
            "source": source,
            "timestamp": datetime.utcnow()
        })
        logging.info(f"‚úì Taux archiv√©: {base}/{target} = {rate}")
    except Exception as e:
        logging.error(f"Erreur Archivage: {e}")

# --- API ENDPOINTS ---

@app.get("/")
def health():
    """
    Health check endpoint avec statistiques syst√®me

    TODO: Ajouter monitoring plus avanc√©:
    - M√©triques Prometheus
    - Tracing distribu√© (Jaeger)
    - Alerting (PagerDuty, Slack)
    - Uptime Robot
    """
    cache_stats = {
        "entries": len(RATE_CACHE),
        "ttl_seconds": CACHE_TTL,
        "type": "in-memory (TODO: migrer vers Redis)"
    }

    db_status = "connected" if rates_collection is not None else "offline"
    if rates_collection is not None:
        try:
            # Compter le nombre de taux archiv√©s
            total_rates = rates_collection.count_documents({})
            cache_stats["archived_rates"] = total_rates
        except:
            pass

    return {
        "system": "SarfX Core AI Engine",
        "version": "2.0.0",
        "status": "operational",
        "database": db_status,
        "cache": cache_stats,
        "features": {
            "ml_models": ["ARIMA", "Prophet"],
            "rate_sources": ["Frankfurter", "Yahoo Finance"],
            "arbitrage": True,
            "real_time_cache": True
        },
        "timestamp": datetime.utcnow().isoformat()
    }

@app.get("/smart-rate/{base}/{target}")
def smart_rate_endpoint(base: str, target: str, amount: float = 1000, background_tasks: BackgroundTasks = None):
    try:
        # 1. Calcul Arbitrage
        arb = calculate_best_execution(base, target, amount)

        # 2. Analyse IA (Timing)
        signal = generate_ai_signal(f"{base}{target}")

        # 3. Archivage asynchrone
        if background_tasks:
            background_tasks.add_task(archive_rate, base, target, arb['rates']['sarfx'], "sarfx_engine")

        return {
            "meta": {
                "pair": f"{base}/{target}",
                "timestamp": datetime.utcnow().isoformat()
            },
            "sarfx_offer": {
                "rate": round(arb['rates']['sarfx'], 4),
                "final_amount": round(amount * arb['rates']['sarfx'], 2),
                "fees": "Inclus (0.5%)"
            },
            "market_intelligence": {
                "bank_rate": round(arb['rates']['bank'], 4),
                "market_rate": round(arb['rates']['market'], 4),
                "best_liquidity_source": arb['best_source'],
                "savings": round(arb['savings'], 2)
            },
            "ai_advisor": {
                "signal": signal,
                "confidence": "Haut" if signal != "NEUTRE" else "Moyen"
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/predict/{pair}")
def predict_endpoint(pair: str):
    """
    Endpoint de pr√©diction ML avec ARIMA + Prophet

    Retourne les pr√©dictions pour les 7 prochains jours avec:
    - ARIMA: Mod√®le classique statistique
    - Prophet: Mod√®le moderne de Meta/Facebook
    - Ensemble: Moyenne des deux pour plus de robustesse

    TODO: Ajouter:
    - LSTM (Deep Learning) pour captures patterns complexes
    - XGBoost avec features engineering (volumes, volatilit√©, etc.)
    - Intervalles de confiance (95%) pour chaque pr√©diction
    - Backtesting automatique pour validation
    """
    try:
        # T√©l√©charger l'historique (1 an pour meilleur entra√Ænement)
        ticker = pair if "=" in pair else f"{pair}=X"
        logging.info(f"üìä T√©l√©chargement donn√©es pour {ticker}...")
        df = yf.download(ticker, period="1y", interval="1d", progress=False)

        if df.empty:
            raise HTTPException(status_code=404, detail=f"Aucune donn√©e disponible pour {pair}")

        current = df['Close'].iloc[-1]
        logging.info(f"‚úì Taux actuel {pair}: {current:.4f}")

        # Entra√Æner les mod√®les
        logging.info("ü§ñ Entra√Ænement ARIMA...")
        arima_model = train_arima_model(df)

        logging.info("ü§ñ Entra√Ænement Prophet...")
        prophet_model = train_prophet_model(df)

        # Pr√©dictions
        steps = 7
        dates = [(df.index[-1] + timedelta(days=i)).strftime("%Y-%m-%d") for i in range(1, steps + 1)]

        # ARIMA
        arima_predictions = None
        if arima_model:
            arima_predictions = predict_with_arima(arima_model, steps)
            logging.info(f"‚úì Pr√©dictions ARIMA g√©n√©r√©es: {len(arima_predictions)} jours")

        # Prophet
        prophet_predictions = None
        if prophet_model:
            prophet_predictions = predict_with_prophet(prophet_model, steps)
            logging.info(f"‚úì Pr√©dictions Prophet g√©n√©r√©es: {len(prophet_predictions)} jours")

        # Ensemble (moyenne des deux mod√®les)
        ensemble_predictions = []
        if arima_predictions and prophet_predictions:
            ensemble_predictions = [
                (a + p) / 2 for a, p in zip(arima_predictions, prophet_predictions)
            ]
            logging.info("‚úì Ensemble cr√©√© (moyenne ARIMA + Prophet)")
        elif arima_predictions:
            ensemble_predictions = arima_predictions
            logging.warning("Prophet √©chec, utilisation ARIMA seul")
        elif prophet_predictions:
            ensemble_predictions = prophet_predictions
            logging.warning("ARIMA √©chec, utilisation Prophet seul")
        else:
            # Fallback: pr√©diction na√Øve (tendance lin√©aire simple)
            logging.warning("Tous les mod√®les ont √©chou√©, fallback sur tendance lin√©aire")
            ensemble_predictions = np.linspace(current, current * 1.01, steps).tolist()

        # Historique r√©cent (30 derniers jours)
        history = df.tail(30).reset_index()
        history_data = [
            {
                'Date': row['Date'].strftime('%Y-%m-%d'),
                'Close': float(row['Close'])
            }
            for _, row in history.iterrows()
        ]

        return {
            "meta": {
                "pair": pair,
                "current_rate": float(current),
                "prediction_days": steps,
                "models_used": ["ARIMA", "Prophet"],
                "timestamp": datetime.utcnow().isoformat()
            },
            "predictions": {
                "dates": dates,
                "Ensemble_Mean": ensemble_predictions,
                "ARIMA": arima_predictions if arima_predictions else ensemble_predictions,
                "Prophet": prophet_predictions if prophet_predictions else ensemble_predictions
            },
            "history": history_data,
            "confidence": "High" if (arima_predictions and prophet_predictions) else "Medium"
        }
    except HTTPException:
        raise
    except Exception as e:
        logging.error(f"Erreur pr√©diction pour {pair}: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/cache/clear")
def clear_cache():
    """
    Endpoint pour vider le cache manuellement

    Utile pour:
    - Forcer le rafra√Æchissement des taux
    - Debug/troubleshooting
    - Maintenance

    TODO: Ajouter authentification (API key) pour s√©curiser cet endpoint
    """
    global RATE_CACHE
    old_size = len(RATE_CACHE)
    RATE_CACHE = {}
    logging.info(f"üßπ Cache vid√©: {old_size} entr√©es supprim√©es")
    return {
        "success": True,
        "message": f"Cache cleared ({old_size} entries removed)",
        "timestamp": datetime.utcnow().isoformat()
    }

@app.get("/cache/stats")
def cache_statistics():
    """
    Statistiques d√©taill√©es du cache

    TODO: Ajouter m√©triques:
    - Hit rate (% de requ√™tes servies par cache)
    - Temps moyen de r√©ponse (cache vs API)
    - Taux de rafra√Æchissement
    """
    stats = []
    for key, data in RATE_CACHE.items():
        age = (datetime.utcnow() - data['timestamp']).total_seconds()
        stats.append({
            "key": key,
            "rate": data['rate'],
            "age_seconds": round(age, 2),
            "expires_in": round(CACHE_TTL - age, 2) if age < CACHE_TTL else 0,
            "is_valid": age < CACHE_TTL
        })

    return {
        "total_entries": len(RATE_CACHE),
        "ttl_seconds": CACHE_TTL,
        "entries": stats,
        "timestamp": datetime.utcnow().isoformat()
    }

if __name__ == "__main__":
    port = int(os.environ.get("PORT", AI_PORT))
    logging.info(f"üöÄ [SarfX AI Backend] D√©marrage sur le port {port}...")
    logging.info(f"üìä Mod√®les ML disponibles: ARIMA, Prophet")
    logging.info(f"üíæ Base de donn√©es: {'‚úì Connect√©e' if db is not None else '‚úó Hors ligne'}")
    logging.info(f"‚ö° Cache en m√©moire activ√© (TTL: {CACHE_TTL}s)")
    print(f"\n{'='*60}")
    print(f"  SarfX AI Backend v2.0 - Ready!")
    print(f"  Swagger Docs: http://localhost:{port}/docs")
    print(f"{'='*60}\n")
    uvicorn.run(app, host="0.0.0.0", port=port)